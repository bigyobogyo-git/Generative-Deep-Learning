{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783f4d56-1df3-4233-996a-5d1caad65219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 09:42:47.447507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763109767.459083   40420 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763109767.462775   40420 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763109767.472821   40420 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763109767.472831   40420 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763109767.472832   40420 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763109767.472833   40420 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-14 09:42:47.475671: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "pattern_wav_name = re.compile(r'([^/\\\\\\.]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59eab252-3c03-49d2-8f47-e39ed44bde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class SpeechFeatureEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b44a5b-67f0-433e-8f84-59270e0b8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36db6f27-4521-4c28-9558-3a1afbaa41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb489ba-4e55-44a2-b506-cb072271b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source)\n",
    "        y = self.decode(x, target)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "            loss = self.compute_loss(y=one_hot, y_pred=preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "        loss = self.compute_loss(y=one_hot, y_pred=preds, sample_weight=mask)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb8d93eb-47ee-484c-89e9-f6403934e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-14 09:58:44--  https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
      "Resolving data.keithito.com (data.keithito.com)... 2400:52e0:1e00::1055:1, 169.150.247.34\n",
      "Connecting to data.keithito.com (data.keithito.com)|2400:52e0:1e00::1055:1|:443... failed: Connection refused.\n",
      "Connecting to data.keithito.com (data.keithito.com)|169.150.247.34|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2748572632 (2,6G) [text/plain]\n",
      "Saving to: ‘LJSpeech-1.1.tar.bz2’\n",
      "\n",
      "LJSpeech-1.1.tar.bz 100%[===================>]   2,56G  40,7MB/s    in 79s     \n",
      "\n",
      "2025-11-14 10:00:09 (33,4 MB/s) - ‘LJSpeech-1.1.tar.bz2’ saved [2748572632/2748572632]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e365359-b27f-4c8c-bbf4-3de3b785fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.utils.get_file(\n",
    "    #os.path.join(os.getcwd(), \"data.tar.gz\"),\n",
    "    #\"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
    "    #extract=True,\n",
    "    #archive_format=\"tar\",\n",
    "    #cache_dir=\".\",\n",
    "#)\n",
    "\n",
    "\n",
    "saveto = \"./datasets/LJSpeech-1.1\"\n",
    "wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n",
    "\n",
    "id_to_text = {}\n",
    "with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        id = line.strip().split(\"|\")[0]\n",
    "        text = line.strip().split(\"|\")[2]\n",
    "        id_to_text[id] = text\n",
    "\n",
    "\n",
    "def get_data(wavs, id_to_text, maxlen=50):\n",
    "    \"\"\"returns mapping of audio paths and transcription texts\"\"\"\n",
    "    data = []\n",
    "    for w in wavs:\n",
    "        id = pattern_wav_name.split(w)[-4]\n",
    "        if len(id_to_text[id]) < maxlen:\n",
    "            data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93f28111-d36d-42e8-843d-54ce8843949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763111806.198989   40420 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8981 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab\n",
    "\n",
    "\n",
    "max_target_len = 200  # all transcripts in out data are < 200 characters\n",
    "data = get_data(wavs, id_to_text, max_target_len)\n",
    "vectorizer = VectorizeChar(max_target_len)\n",
    "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "\n",
    "\n",
    "def create_text_ds(data):\n",
    "    texts = [_[\"text\"] for _ in data]\n",
    "    text_ds = [vectorizer(t) for t in texts]\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
    "    return text_ds\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    # spectrogram using stft\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
    "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
    "    # normalisation\n",
    "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
    "    x = (x - means) / stddevs\n",
    "    audio_len = tf.shape(x)[0]\n",
    "    # padding to 10 seconds\n",
    "    pad_len = 2754\n",
    "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
    "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_audio_ds(data):\n",
    "    flist = [_[\"audio\"] for _ in data]\n",
    "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
    "    audio_ds = audio_ds.map(path_to_audio, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return audio_ds\n",
    "\n",
    "\n",
    "def create_tf_dataset(data, bs=4):\n",
    "    audio_ds = create_audio_ds(data)\n",
    "    text_ds = create_text_ds(data)\n",
    "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
    "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
    "    ds = ds.batch(bs)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "split = int(len(data) * 0.99)\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]\n",
    "ds = create_tf_dataset(train_data, bs=64)\n",
    "val_ds = create_tf_dataset(test_data, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba967e6b-bff3-46d9-a405-1e25de083962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch containing the keys \"source\" and \"target\"\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        source = self.batch[\"source\"]\n",
    "        target = self.batch[\"target\"].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:     {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59140119-1954-4922-b187-dfb0d5136983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_lr=0.00001,\n",
    "        lr_after_warmup=0.001,\n",
    "        final_lr=0.00001,\n",
    "        warmup_epochs=15,\n",
    "        decay_epochs=85,\n",
    "        steps_per_epoch=203,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_after_warmup = lr_after_warmup\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def calculate_lr(self, epoch):\n",
    "        \"\"\"linear warm up - linear decay\"\"\"\n",
    "        warmup_lr = (\n",
    "            self.init_lr\n",
    "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
    "        )\n",
    "        decay_lr = tf.math.maximum(\n",
    "            self.final_lr,\n",
    "            self.lr_after_warmup\n",
    "            - (epoch - self.warmup_epochs)\n",
    "            * (self.lr_after_warmup - self.final_lr)\n",
    "            / self.decay_epochs,\n",
    "        )\n",
    "        return tf.math.minimum(warmup_lr, decay_lr)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = step // self.steps_per_epoch\n",
    "        epoch = tf.cast(epoch, \"float32\")\n",
    "        return self.calculate_lr(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04c18f16-5385-45d4-b6a9-db901833ce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - loss: 1.8211target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <the e ithe the an an the an tin t atothe the t the t as ce an the t t a as the at ator o athe an athe the a an o as the te the pre the the the and t t the he at tos tone the aone rs tonof the on te s\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <the e ithe the an an the an tin t atothe the t the t as ce an the t t a as the at ator o athe an athe the a an o as the te the pre the the the and he alite on t tos tone the aone rs tonof the on te s\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <the e ithe the an an the an tin t atothe the t the t as ce an the t t a as the at ator o athe an athe the a an o as the te the pre the the the and he alite on t tos tone the aone rs tonof the on te s\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <the e ithe the an an the an tin t atothe the t the t as ce an the t t a as the at ator o athe an athe the a an o as the te the pre the the the and t t the he at tos tone the aone rs tonof the on te s\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 287ms/step - loss: 1.6516 - val_loss: 1.5143\n",
      "Epoch 2/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 249ms/step - loss: 1.3607 - val_loss: 1.3705\n",
      "Epoch 3/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 270ms/step - loss: 1.3170 - val_loss: 1.3500\n",
      "Epoch 4/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 271ms/step - loss: 1.3045 - val_loss: 1.3417\n",
      "Epoch 5/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 271ms/step - loss: 1.2935 - val_loss: 1.3289\n",
      "Epoch 6/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 1.2771target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <thice prestion the the the pre pre the prese the prenof the pre pre thenofofion the the the the the the the the pre the the pre the the pre the.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald the the the the osssin the the of ossssassinof on asion the assin on the the the of the the the the the.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the she the the the the she she the there the the scof the theshe thereareshespariof the>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <the re wande the the the the the wanore the the the the the the wand the the wand the re the the the the tre wanore the the the ande the.>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 290ms/step - loss: 1.2680 - val_loss: 1.2978\n",
      "Epoch 7/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 271ms/step - loss: 1.2252 - val_loss: 1.2392\n",
      "Epoch 8/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 276ms/step - loss: 1.1434 - val_loss: 1.1223\n",
      "Epoch 9/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 275ms/step - loss: 0.9965 - val_loss: 0.9544\n",
      "Epoch 10/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 275ms/step - loss: 0.8617 - val_loss: 0.8649\n",
      "Epoch 11/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.8003target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massical of suppace accoless chapace apapace accolon and the man reagementation and the man rean regementar cressssthe ch.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <asssination when on the day oswald was was was wald on the day on the day oswald was was was levision.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the acker the acker the acker the ackerinshmance,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <there taked been been been been been been been been been been been been been proper rober rober rober rober rober rof cun>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 293ms/step - loss: 0.7827 - val_loss: 0.8080\n",
      "Epoch 12/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 272ms/step - loss: 0.7369 - val_loss: 0.7729\n",
      "Epoch 13/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 278ms/step - loss: 0.7016 - val_loss: 0.7539\n",
      "Epoch 14/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 278ms/step - loss: 0.6739 - val_loss: 0.7445\n",
      "Epoch 15/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 278ms/step - loss: 0.6563 - val_loss: 0.7412\n",
      "Epoch 16/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.6464target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massion and tand the massion and the massion and the mass supression and tand tand the mass tolen supressior to alle allllion son atheng h.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <on the day oswald was watching television when shootin when shootin when shootin when shootin when shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the actrerience for the punishments about to experience,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <there which tim by which tim where ight timeraupted by which tim which tim by which timby which timby which timby which tin t,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 295ms/step - loss: 0.6365 - val_loss: 0.7103\n",
      "Epoch 17/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 272ms/step - loss: 0.6145 - val_loss: 0.6964\n",
      "Epoch 18/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 278ms/step - loss: 0.5984 - val_loss: 0.6826\n",
      "Epoch 19/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 278ms/step - loss: 0.5849 - val_loss: 0.6844\n",
      "Epoch 20/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.5728 - val_loss: 0.6736\n",
      "Epoch 21/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5651target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massion and the masssion and the massion and the massion succle supprossion and the massixion and the massion stolin sin sth,>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <on the dayve the assasssination when shootin when shootin when shootin when shootin when shootin.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the ackrationshments about to too to to to to too too to to to too to too to too to took to to to too to to too to th se>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they they which time which time which time which time which time which tin procuce hand been procured,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 296ms/step - loss: 0.5625 - val_loss: 0.6713\n",
      "Epoch 22/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 276ms/step - loss: 0.5533 - val_loss: 0.6725\n",
      "Epoch 23/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.5443 - val_loss: 0.6687\n",
      "Epoch 24/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.5365 - val_loss: 0.6631\n",
      "Epoch 25/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.5279 - val_loss: 0.6586\n",
      "Epoch 26/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 0.5207target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massermination and the indiviscton and the indiviscton under creship ackle of stollen, and the indivisctermination of stolef,>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <on the day vessination when shoting. marina on the day oswald was watching.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the apprration she was about to to the punishments about to to the punis>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they which time by which time by which time by which time by which time by which time by which time which tin procured,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 297ms/step - loss: 0.5161 - val_loss: 0.6315\n",
      "Epoch 27/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 273ms/step - loss: 0.4970 - val_loss: 0.6169\n",
      "Epoch 28/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 279ms/step - loss: 0.4815 - val_loss: 0.5947\n",
      "Epoch 29/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 278ms/step - loss: 0.4669 - val_loss: 0.5880\n",
      "Epoch 30/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.4586 - val_loss: 0.5829\n",
      "Epoch 31/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4542target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass emass ixtolling so pressixchould supprussion and the indevitual suppruschop accle supricacle supricacle suppression and the inder creson and rere.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <on the day of the assassination on the day of the assassination whensh learned of the shoting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the apprrationshman she was about to to ook sperience,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they the the fresh, hand broks hand broks hand broks hand stronger roks hand brokesh and brokesh and broks hand broks hand stronger>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 295ms/step - loss: 0.4530 - val_loss: 0.5853\n",
      "Epoch 32/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 275ms/step - loss: 0.4561 - val_loss: 0.5759\n",
      "Epoch 33/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.4472 - val_loss: 0.5663\n",
      "Epoch 34/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.4363 - val_loss: 0.5662\n",
      "Epoch 35/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 279ms/step - loss: 0.4309 - val_loss: 0.5669\n",
      "Epoch 36/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4298target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass epression and regimentation and regimentation and regimentation and regimentation under crustion and regimentation on and regimentation and regime>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <on the day of the assassination whensh  elevision whensh  eshe levision whensh  elevision whensh oting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <were the appratis for the punishmat she was about to tookspery ince,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they which tin back to jail, by which tin the afternonger roks hand stronger roks hand stronger roks hand broks hand brok surd,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 296ms/step - loss: 0.4283 - val_loss: 0.5577\n",
      "Epoch 37/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 273ms/step - loss: 0.4290 - val_loss: 0.5563\n",
      "Epoch 38/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 284ms/step - loss: 0.4231 - val_loss: 0.5543\n",
      "Epoch 39/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 279ms/step - loss: 0.4186 - val_loss: 0.5536\n",
      "Epoch 40/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 278ms/step - loss: 0.4144 - val_loss: 0.5560\n",
      "Epoch 41/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4141target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massixtelling and regimination under cruse pressipssion and regimination undercrushiet.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was watch intelevision when she larned of the shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <were the appratisf or the punis,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they were gan were a gan were a gan were a gan were a gan were a gain procured,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 296ms/step - loss: 0.4125 - val_loss: 0.5538\n",
      "Epoch 42/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 275ms/step - loss: 0.4091 - val_loss: 0.5565\n",
      "Epoch 43/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.4047 - val_loss: 0.5592\n",
      "Epoch 44/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 279ms/step - loss: 0.4018 - val_loss: 0.5547\n",
      "Epoch 45/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3998 - val_loss: 0.5485\n",
      "Epoch 46/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.3972target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass extolin, and the indivitual supression and regimenations and regimentations and regimentation under cresial suel sue shite.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was watch intell vision when she levision when she levision when she levision whenshe levision when she levision,>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the apratisference,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they wittogen back to jail, by which time been propes huns stronger ropsured, and were a gen broker ropsured,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 298ms/step - loss: 0.3982 - val_loss: 0.5518\n",
      "Epoch 47/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 277ms/step - loss: 0.3943 - val_loss: 0.5535\n",
      "Epoch 48/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3901 - val_loss: 0.5539\n",
      "Epoch 49/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3873 - val_loss: 0.5564\n",
      "Epoch 50/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 279ms/step - loss: 0.3871 - val_loss: 0.5596\n",
      "Epoch 51/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3868target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massix stemination supression and regimantion and regimantion and regimantion and regimantation under cresion and regimantion sough#>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <on the day of the shooting television when shooting television when she larned of the shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the apperatis, for the punishmat she was about to ksperience,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they witted tiken back to gen back to gem and broker roks hand stronger a a gail, and were agail, by which tin the afternooon and back ail,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 296ms/step - loss: 0.3863 - val_loss: 0.5503\n",
      "Epoch 52/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 276ms/step - loss: 0.3835 - val_loss: 0.5544\n",
      "Epoch 53/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3812 - val_loss: 0.5607\n",
      "Epoch 54/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3790 - val_loss: 0.5623\n",
      "Epoch 55/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3777 - val_loss: 0.5561\n",
      "Epoch 56/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3765target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massix tole of stolin and the indivitual suppression and the indivitual suppression and the indivitual suppression and the indivite.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was watchingtell vision when she larned of the she larned of the assassination when she larned of the she larned of the shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the appunishman she was about tooks ponishman she was about tooks pirience>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they wittagen back in by which tin the aftirnooks hand stronger a genewhich tin back to jail, by which and broks hand brokeand brooksured,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 297ms/step - loss: 0.3755 - val_loss: 0.5564\n",
      "Epoch 57/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 274ms/step - loss: 0.3755 - val_loss: 0.5549\n",
      "Epoch 58/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 290ms/step - loss: 0.3732 - val_loss: 0.5556\n",
      "Epoch 59/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 287ms/step - loss: 0.3703 - val_loss: 0.5558\n",
      "Epoch 60/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3674 - val_loss: 0.5570\n",
      "Epoch 61/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3664target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massix stoling and regimenation sundercresion and regimenation sundercresion and regimation s oundercression and the indivit,>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was wald was watching television when shooting television when shooting,>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the appration she was about to experience,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they courd, out in the afternoon, and were a gend bropes hand stronger a gend bropes hand stronger roper ropesh, hand stronger>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 296ms/step - loss: 0.3655 - val_loss: 0.5610\n",
      "Epoch 62/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 276ms/step - loss: 0.3653 - val_loss: 0.5574\n",
      "Epoch 63/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3651 - val_loss: 0.5584\n",
      "Epoch 64/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3618 - val_loss: 0.5598\n",
      "Epoch 65/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3600 - val_loss: 0.5588\n",
      "Epoch 66/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3594target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the massix stemination supression undercression and regimentations ounder cregimenations and regimentations ouk pression and the indivit#>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was wald was wald was wald was wald was wald was watching television when shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the apperatus for the punish mad she was about took#sperience>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they witaken backen and bropesh, and were a gend bropesh, hand stronger a gen backen and were a gen backed ail, by which tin procured.>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 296ms/step - loss: 0.3588 - val_loss: 0.5580\n",
      "Epoch 67/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 277ms/step - loss: 0.3577 - val_loss: 0.5588\n",
      "Epoch 68/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3562 - val_loss: 0.5617\n",
      "Epoch 69/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 279ms/step - loss: 0.3552 - val_loss: 0.5620\n",
      "Epoch 70/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3545 - val_loss: 0.5640\n",
      "Epoch 71/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 0.3541target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass #apression and regimantion supression and the indivitual supression and the indivitual supression and the massix stemintation suk pressiof#>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was wald was wald was wald was wald was wald was waching television when shooting television when shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the appratis for the punish man she was about to experience>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they witaken brooks hand were a gend brookes hand were a gend brooks hand were a gend fresh, hand broought out in the afterno>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 297ms/step - loss: 0.3539 - val_loss: 0.5652\n",
      "Epoch 72/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 275ms/step - loss: 0.3529 - val_loss: 0.5659\n",
      "Epoch 73/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3519 - val_loss: 0.5698\n",
      "Epoch 74/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3507 - val_loss: 0.5702\n",
      "Epoch 75/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 279ms/step - loss: 0.3495 - val_loss: 0.5673\n",
      "Epoch 76/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.3487target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass ixtemination supression and regimenation supression and the indivitual sunder cression and the indivitual sundercrewination sundercrucruce chiph.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was che larned of the shootin, marina oswald was wald was che larned of the shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the appratus for the punish mashe was about to ksperience,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they we tout in back to jail by which tin back to jail by which tin back to jail by which tin back to jail, by which tin the afterno.>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 298ms/step - loss: 0.3483 - val_loss: 0.5665\n",
      "Epoch 77/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 274ms/step - loss: 0.3472 - val_loss: 0.5665\n",
      "Epoch 78/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3461 - val_loss: 0.5660\n",
      "Epoch 79/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3453 - val_loss: 0.5670\n",
      "Epoch 80/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3440 - val_loss: 0.5684\n",
      "Epoch 81/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 0.3434target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass ixtemination souk pression undercrew to pression and the indivitual suprcession undercrew to pression and the indivitual suke pression and the indifffe>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was wald was wald was wald was wald was chingtell vision when shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the appratus for the punish was about tooks perience>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they wour a gen back to jail by which tin back to jail by which tin back to jail by which tin the afternoonger ropesh, hand stronger,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 297ms/step - loss: 0.3429 - val_loss: 0.5679\n",
      "Epoch 82/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 277ms/step - loss: 0.3420 - val_loss: 0.5702\n",
      "Epoch 83/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3415 - val_loss: 0.5729\n",
      "Epoch 84/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3407 - val_loss: 0.5720\n",
      "Epoch 85/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 282ms/step - loss: 0.3400 - val_loss: 0.5719\n",
      "Epoch 86/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.3396target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass #pression and regimantion souct pression under cret to paccle of stemination souk pression and the indivitual supression and regimantion souk press#>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was wald was watching television on the day of the assassination, marina oswald was wald was watching.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the apperatus for the punish man she was about to experience>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they wouln by which tin back to jail by which tin brooks hand were a gen brook#s have been procured,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 298ms/step - loss: 0.3394 - val_loss: 0.5729\n",
      "Epoch 87/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 276ms/step - loss: 0.3388 - val_loss: 0.5710\n",
      "Epoch 88/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3382 - val_loss: 0.5739\n",
      "Epoch 89/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 281ms/step - loss: 0.3378 - val_loss: 0.5780\n",
      "Epoch 90/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 282ms/step - loss: 0.3384 - val_loss: 0.5775\n",
      "Epoch 91/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 0.3394target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass ixtemination sounder cret to pression and the indivitual suprcression and the indivitual sunder cretipacle of stemination sounder cregimantions and#>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was wald was wald was wald was wald was wald was wald was wald was wald was wald was wald was wald was wald was atching.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the apprat she was about tooksperience,>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they would in back to jail by which tin back to jail by which tin the afternoo t in the afternoon by which tin the afterno>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 297ms/step - loss: 0.3387 - val_loss: 0.5782\n",
      "Epoch 92/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 275ms/step - loss: 0.3373 - val_loss: 0.5768\n",
      "Epoch 93/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3365 - val_loss: 0.5768\n",
      "Epoch 94/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3360 - val_loss: 0.5765\n",
      "Epoch 95/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3358 - val_loss: 0.5781\n",
      "Epoch 96/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.3357target:     <with the mass extermination so typical of stalin, and the individual suppression and regimentation under khrushchev.>\n",
      "prediction: <with the mass #pression under cretipacle of stemination sounder cression under cretion and the indivitual supression and the indivitual sunder cressioff.>\n",
      "\n",
      "target:     <on the day of the assassination, marina oswald was watching television when she learned of the shooting.>\n",
      "prediction: <oswald was wald was wald was wald was wald was wald was watching television when shooting.>\n",
      "\n",
      "target:     <where the apparatus for the punishment she was about to experience>\n",
      "prediction: <where the apprat she was about to knosh mant she was about tooksperience>\n",
      "\n",
      "target:     <they were taken back to jail, and were again brought out in the afternoon, by which time fresh and stronger ropes had been procured,>\n",
      "prediction: <they woul by which tin back to jail, by which tin back to jail, by which tin the afterno rokeurd,>\n",
      "\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 298ms/step - loss: 0.3356 - val_loss: 0.5793\n",
      "Epoch 97/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 274ms/step - loss: 0.3354 - val_loss: 0.5805\n",
      "Epoch 98/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 279ms/step - loss: 0.3353 - val_loss: 0.5810\n",
      "Epoch 99/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - loss: 0.3352 - val_loss: 0.5825\n",
      "Epoch 100/100\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 296ms/step - loss: 0.3351 - val_loss: 0.5822\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_ds))\n",
    "\n",
    "# The vocabulary to convert predicted indices into characters\n",
    "idx_to_char = vectorizer.get_vocabulary()\n",
    "display_cb = DisplayOutputs(\n",
    "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
    ")  # set the arguments as per vocabulary index for '<' and '>'\n",
    "\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=2,\n",
    "    num_feed_forward=400,\n",
    "    target_maxlen=max_target_len,\n",
    "    num_layers_enc=4,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=34,\n",
    ")\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    label_smoothing=0.1,\n",
    ")\n",
    "\n",
    "learning_rate = CustomSchedule(\n",
    "    init_lr=0.00001,\n",
    "    lr_after_warmup=0.001,\n",
    "    final_lr=0.00001,\n",
    "    warmup_epochs=15,\n",
    "    decay_epochs=85,\n",
    "    steps_per_epoch=len(ds),\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add4ba5-56e1-4baf-9868-3a548110486d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e734bb8-c647-4740-9534-39bcd48c0a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58489fd8-d3cd-41f3-99d7-bb6f755d7c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cde17b-8f79-422f-b408-a14b24f0b022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d02e81-d69b-4eb2-a747-3de12fa002fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fa085-7355-4888-aa25-57655ca4c82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a470e0-abc3-4738-9c90-52e7100caf22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559334da-351b-489e-a712-1746138f6558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540cb1ca-adaf-4329-9112-1b3e01434f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c3709-5d04-4b82-a1dc-89b586d80dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef9118-8675-411a-8138-803f243609be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3aa2e-2600-473e-933f-269c5c211b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb905fe-b041-45f8-8730-908248ffadae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247d01e-5bde-4495-a922-fd0a270f9f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
