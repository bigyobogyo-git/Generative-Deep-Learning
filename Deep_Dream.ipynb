{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f130deec-5eeb-478c-908f-ceb362eb314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications import inception_v3\n",
    "\n",
    "base_image_path = './aGBdQyK.jpeg'\n",
    "#base_image_path = keras.utils.get_file(\"sky.jpg\", \"https://i.imgur.com/aGBdQyK.jpeg\")\n",
    "result_prefix = \"sky_dream\"\n",
    "\n",
    "# These are the names of the layers\n",
    "# for which we try to maximize activation,\n",
    "# as well as their weight in the final loss\n",
    "# we try to maximize.\n",
    "# You can tweak these setting to obtain new visual effects.\n",
    "layer_settings = {\n",
    "    \"mixed4\": 1.0,\n",
    "    \"mixed5\": 1.5,\n",
    "    \"mixed6\": 2.0,\n",
    "    \"mixed7\": 2.5,\n",
    "}\n",
    "\n",
    "# Playing with these hyperparameters will also allow you to achieve new effects\n",
    "step = 0.01  # Gradient ascent step size\n",
    "num_octave = 3  # Number of scales at which to run gradient ascent\n",
    "octave_scale = 1.4  # Size ratio between scales\n",
    "iterations = 20  # Number of ascent steps per scale\n",
    "max_loss = 15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e480e69-c7b6-4bb0-80a0-551327209fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(base_image_path)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bbc5cc2-45b7-424c-accb-2771ded07a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures\n",
    "    # into appropriate arrays.\n",
    "    img = keras.utils.load_img(image_path)\n",
    "    img = keras.utils.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a NumPy array into a valid image.\n",
    "    x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    # Undo inception v3 preprocessing\n",
    "    x /= 2.0\n",
    "    x += 0.5\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ae54f20-f18c-482d-9b1f-62c9bac729e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762882915.503578   22137 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8833 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Build an InceptionV3 model loaded with pre-trained ImageNet weights\n",
    "model = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict(\n",
    "    [\n",
    "        (layer.name, layer.output)\n",
    "        for layer in [model.get_layer(name) for name in layer_settings.keys()]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set up a model that returns the activation values for every target layer\n",
    "# (as a dict)\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36300900-f0ff-46ed-8d69-90f50dd9857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input_image):\n",
    "    features = feature_extractor(input_image)\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "    for name in features.keys():\n",
    "        coeff = layer_settings[name]\n",
    "        activation = features[name]\n",
    "        # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "        scaling = tf.reduce_prod(tf.cast(tf.shape(activation), \"float32\"))\n",
    "        loss += coeff * tf.reduce_sum(tf.square(activation[:, 2:-2, 2:-2, :])) / scaling\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbe7b733-228c-439b-a8a4-fd8d16855808",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_ascent_step(img, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img)\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, img)\n",
    "    # Normalize gradients.\n",
    "    grads /= tf.maximum(tf.reduce_mean(tf.abs(grads)), 1e-6)\n",
    "    img += learning_rate * grads\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def gradient_ascent_loop(img, iterations, learning_rate, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss, img = gradient_ascent_step(img, learning_rate)\n",
    "        if max_loss is not None and loss > max_loss:\n",
    "            break\n",
    "        print(\"... Loss value at step %d: %.2f\" % (i, loss))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71ab5e58-65ca-4d28-bc65-7be1c2a7bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing octave 0 with shape (326, 489)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 18:42:30.201043: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Loss value at step 0: 0.45\n",
      "... Loss value at step 1: 0.63\n",
      "... Loss value at step 2: 0.91\n",
      "... Loss value at step 3: 1.24\n",
      "... Loss value at step 4: 1.58\n",
      "... Loss value at step 5: 1.90\n",
      "... Loss value at step 6: 2.23\n",
      "... Loss value at step 7: 2.51\n",
      "... Loss value at step 8: 2.82\n",
      "... Loss value at step 9: 3.11\n",
      "... Loss value at step 10: 3.41\n",
      "... Loss value at step 11: 3.67\n",
      "... Loss value at step 12: 3.96\n",
      "... Loss value at step 13: 4.22\n",
      "... Loss value at step 14: 4.51\n",
      "... Loss value at step 15: 4.73\n",
      "... Loss value at step 16: 4.99\n",
      "... Loss value at step 17: 5.24\n",
      "... Loss value at step 18: 5.44\n",
      "... Loss value at step 19: 5.70\n",
      "Processing octave 1 with shape (457, 685)\n",
      "... Loss value at step 0: 1.10\n",
      "... Loss value at step 1: 1.76\n",
      "... Loss value at step 2: 2.33\n",
      "... Loss value at step 3: 2.81\n",
      "... Loss value at step 4: 3.27\n",
      "... Loss value at step 5: 3.67\n",
      "... Loss value at step 6: 4.06\n",
      "... Loss value at step 7: 4.46\n",
      "... Loss value at step 8: 4.84\n",
      "... Loss value at step 9: 5.22\n",
      "... Loss value at step 10: 5.57\n",
      "... Loss value at step 11: 5.89\n",
      "... Loss value at step 12: 6.21\n",
      "... Loss value at step 13: 6.56\n",
      "... Loss value at step 14: 6.87\n",
      "... Loss value at step 15: 7.21\n",
      "... Loss value at step 16: 7.47\n",
      "... Loss value at step 17: 7.79\n",
      "... Loss value at step 18: 8.06\n",
      "... Loss value at step 19: 8.35\n",
      "Processing octave 2 with shape (640, 960)\n",
      "... Loss value at step 0: 1.28\n",
      "... Loss value at step 1: 2.05\n",
      "... Loss value at step 2: 2.66\n",
      "... Loss value at step 3: 3.19\n",
      "... Loss value at step 4: 3.71\n",
      "... Loss value at step 5: 4.17\n",
      "... Loss value at step 6: 4.64\n",
      "... Loss value at step 7: 5.06\n",
      "... Loss value at step 8: 5.48\n",
      "... Loss value at step 9: 5.91\n",
      "... Loss value at step 10: 6.24\n",
      "... Loss value at step 11: 6.58\n",
      "... Loss value at step 12: 6.93\n",
      "... Loss value at step 13: 7.29\n",
      "... Loss value at step 14: 7.62\n",
      "... Loss value at step 15: 7.90\n",
      "... Loss value at step 16: 8.25\n",
      "... Loss value at step 17: 8.53\n",
      "... Loss value at step 18: 8.90\n",
      "... Loss value at step 19: 9.10\n"
     ]
    }
   ],
   "source": [
    "original_img = preprocess_image(base_image_path)\n",
    "original_shape = original_img.shape[1:3]\n",
    "\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale**i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "successive_shapes = successive_shapes[::-1]\n",
    "shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])\n",
    "\n",
    "img = tf.identity(original_img)  # Make a copy\n",
    "for i, shape in enumerate(successive_shapes):\n",
    "    print(\"Processing octave %d with shape %s\" % (i, shape))\n",
    "    img = tf.image.resize(img, shape)\n",
    "    img = gradient_ascent_loop(\n",
    "        img, iterations=iterations, learning_rate=step, max_loss=max_loss\n",
    "    )\n",
    "    upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)\n",
    "    same_size_original = tf.image.resize(original_img, shape)\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "\n",
    "    img += lost_detail\n",
    "    shrunk_original_img = tf.image.resize(original_img, shape)\n",
    "\n",
    "keras.utils.save_img(result_prefix + \".png\", deprocess_image(img.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6119366-fe1e-4eaf-b0d0-b56eb0333894",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "display(Image(result_prefix + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a73c82-e659-42dd-bac3-15f3b782becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(base_image_path)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df7726-92ff-4b2e-ad12-bbc50b80195d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5b9a4-fd17-42f5-b490-96c95f22a021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f808b-ee92-4ae4-8097-f0214a5179bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef629dbd-f8d0-45a4-9e9f-3e2c4ace1348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989948ab-3271-4555-824f-154766a6d499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebcab0-5eee-4101-9bf2-f278883f120c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ab5a4-67bc-48b0-9a13-112b287fcd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f01fa6-8f51-4685-8244-ca44daff1cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33d69d-9b11-451e-8fc5-7cd637d452f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a94d1-1723-4c5b-97f3-23802fa3b66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0925577-8b0e-4e80-a409-79f7ae699d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829ac87-755d-42ab-9242-fa3747d4ff09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7766e20-1785-4fc7-b3a2-ec403ab26255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f10121-6a86-431b-a63b-0e9b53cea45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a33ad-f031-4a79-a3ae-0d06fd1d4ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1a9fe-763e-4172-a6f7-b491062cc416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87605c3-6d3b-4dfb-ada0-d23270edc636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bdd92-efb0-44bf-ab49-935a9e7c1cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293c3d1-d1a9-4de9-aa01-addc8a16d8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
