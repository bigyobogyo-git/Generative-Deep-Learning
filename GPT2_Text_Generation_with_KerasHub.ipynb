{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1192a429-294f-4590-94b5-a1c07c503840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:31:19.495373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762965079.507195   31748 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762965079.510939   31748 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762965079.521468   31748 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762965079.521477   31748 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762965079.521478   31748 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762965079.521479   31748 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-12 17:31:19.524847: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/xy/Desktop/ml/GDL/tf_2.19/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_hub\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a912e79-a0d0-4f15-94b6-ad684192a032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762965082.564774   31748 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8926 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# To speed up training and generation, we use preprocessor of length 128\n",
    "# instead of full length 1024.\n",
    "preprocessor = keras_hub.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=128,\n",
    ")\n",
    "gpt2_lm = keras_hub.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e035abc-501f-498e-bd27-ea5ef8d243aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762965087.489108   31748 service.cc:152] XLA service 0x5ff2a46997b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1762965087.489122   31748 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-11-12 17:31:27.555210: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1762965088.097828   31748 cuda_dnn.cc:529] Loaded cuDNN version 91500\n",
      "I0000 00:00:1762965091.860858   31748 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "My trip to Yosemite was an adventure in the wilderness, a trip that was full of beautiful views and amazing food and drink. It is my favorite part of Yosemite. The views were beautiful as well, with the sun shining over the mountains and the beautiful views. I was able to spend an entire day in the sun. I had the opportunity to visit a couple of different places on the route, which was a nice change of pace for me. It was a good day for hiking and hiking and hiking. I also enjoyed the sun and the view of the mountains. There are some beautiful spots in Yosemite where there are no clouds or clouds. There are a couple of places where there are no clouds or the sun is just above them. It was a nice day for me.\n",
      "\n",
      "I went to Yosemite in the winter, but it is a bit different. The view was great and the views are beautiful. The weather was nice, the weather was good but there was no wind or rain,\n",
      "TOTAL TIME ELAPSED: 7.43s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f05ed0e-6860-4304-9ba9-b2e2b7f88fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "That Italian restaurant is now known as the Italian Restaurant in San Diego, but it wasn't always so. The restaurant was founded in 1887 by the Italian immigrant, and the name changed in 1885 when Italian immigrants started opening up a second Italian restaurant.\n",
      "\n",
      "The Italian Restaurant in San Diego is now known as the Italian Restaurant in San Diego, but it wasn't always so. The restaurant was founded in 1887 by the Irish immigrant, and the name changed in 1885 when Italian immigrants started opening up a second Italian restaurant.\n",
      "\n",
      "\"The first Italian restaurant in San Diego was in the late 18th century. They opened in 1790 and it was a small, small Italian restaurant, so the Italians were the first in the area. They were the only Italians in the area at the time, and they were the only Italian in the whole area.\n",
      "\n",
      "\"It was the first place to have a restaurant in the city of San Diego, and it was the first Italian restaurant\n",
      "TOTAL TIME ELAPSED: 0.91s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7c801e-3a79-49f9-ad44-a16dfc2f8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "reddit_ds = tfds.load(\"reddit_tifu\", split=\"train\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9953906b-bffa-45c4-b2e8-603fe140256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"me and a friend decided to go to the beach last sunday. we loaded up and headed out. we were about half way there when i decided that i was not leaving till i had seafood. \\n\\nnow i'm not talking about red lobster. no friends i'm talking about a low country boil. i found the restaurant and got directions. i don't know if any of you have heard about the crab shack on tybee island but let me tell you it's worth it. \\n\\nwe arrived and was seated quickly. we decided to get a seafood sampler for two and split it. the waitress bought it out on separate platters for us. the amount of food was staggering. two types of crab, shrimp, mussels, crawfish, andouille sausage, red potatoes, and corn on the cob. i managed to finish it and some of my friends crawfish and mussels. it was a day to be a fat ass. we finished paid for our food and headed to the beach. \\n\\nfunny thing about seafood. it runs through me faster than a kenyan \\n\\nwe arrived and walked around a bit. it was about 45min since we arrived at the beach when i felt a rumble from the depths of my stomach. i ignored it i didn't want my stomach to ruin our fun. i pushed down the feeling and continued. about 15min later the feeling was back and stronger than before. again i ignored it and continued. 5min later it felt like a nuclear reactor had just exploded in my stomach. i started running. i yelled to my friend to hurry the fuck up. \\n\\nrunning in sand is extremely hard if you did not know this. we got in his car and i yelled at him to floor it. my stomach was screaming and if he didn't hurry i was gonna have this baby in his car and it wasn't gonna be pretty. after a few red lights and me screaming like a woman in labor we made it to the store. \\n\\ni practically tore his car door open and ran inside. i ran to the bathroom opened the door and barely got my pants down before the dam burst and a flood of shit poured from my ass. \\n\\ni finished up when i felt something wet on my ass. i rubbed it thinking it was back splash. no, mass was covered in the after math of me abusing the toilet. i grabbed all the paper towels i could and gave my self a whores bath right there. \\n\\ni sprayed the bathroom down with the air freshener and left. an elderly lady walked in quickly and closed the door. i was just about to walk away when i heard gag. instead of walking i ran. i got to the car and told him to get the hell out of there.\"\n",
      "b'liking seafood'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:31:33.503295: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-11-12 17:31:33.508949: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for document, title in reddit_ds:\n",
    "    print(document.numpy())\n",
    "    print(title.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef05b350-d590-4806-8b40-54fca134c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    reddit_ds.map(lambda document, _: document)\n",
    "    .batch(10)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "149b1cd5-3a00-4418-b102-34e484838a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:31:51.686919: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m484/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.3079 - loss: 3.4231"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:33:00.212081: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-11-12 17:33:00.212865: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 102ms/step - accuracy: 0.3152 - loss: 3.3633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x740b64234970>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "# Linearly decaying learning rate.\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-5,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558fc892-5229-4674-ba3d-ddd3b907c161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "I like basketball so this was my last time. i had just finished a week at school and had just started to get some work done. i was in my room with my girlfriend and her boyfriend, watching the game at my school. i was watching a movie and i was watching some music, and i saw my girlfriend's friend, who i had just met at a movie. i looked at her and said, \"oh fuck, she's really cute.\" she looked at me and said, \"oh shit, she's really funny.\" i said to her, \"oh shit, she's really funny.\" she looked back at me and said \"oh shit, she's really funny.\" i\n",
      "TOTAL TIME ELAPSED: 5.97s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cb956e-f93a-4ccf-80b3-3da1939678c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "I like basketball. it's a great sport and i like to play it. i was a little worried about how the ball would bounce.  \n",
      "\n",
      "i was playing a basketball game and it was raining. i thought i could hit it with a little force and it would bounce back. i thought it was too big and i hit the ball hard. i thought that would make it bounce back and i hit it with the ball, and it bounced off. \n",
      "\n",
      "the ball bounced back and landed on the floor. the ball hit the floor. the ball landed on the floor and the ball was flying out. i didn't see the ball until i saw a guy in the middle of the floor who looked like he had been hit. \n",
      "\n",
      "i was so pissed.   \n",
      "\n",
      "GPT-2 output:\n",
      "I like basketball, but it's not my thing. \n",
      "\n",
      "so, i'm a freshman in high school, and i'm a freshman in college. \n",
      "\n",
      "so, i'm in the middle of the night, and i'm sitting in the living room, and i'm thinking, \"hey, this is a good idea. \n",
      "\n",
      "so, i'm sitting there, and i'm thinking, \"hey, this is a good idea. \n",
      "\n",
      "so, i'm sitting there, and i'm thinking, \"hey, this is a good idea. \n",
      "\n",
      "so, i'm sitting there, and i'm thinking, \"hey, this is a good idea. \n",
      "\n",
      "so, i'm sitting there, and i'm thinking, \"hey, this is a good idea. \n",
      "\n",
      "so, i'm sitting there, and i'm thinking, \"hey, this is a good idea. \n",
      "\n",
      "so, i'm sitting there, and i'm thinking, \"hey,\n"
     ]
    }
   ],
   "source": [
    "# Use a string identifier.\n",
    "gpt2_lm.compile(sampler=\"top_k\")\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\n",
    "greedy_sampler = keras_hub.samplers.GreedySampler()\n",
    "gpt2_lm.compile(sampler=greedy_sampler)\n",
    "\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca62f6a6-3e75-4e50-9940-2f850ed2f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'chinese-poetry'...\n",
      "remote: Enumerating objects: 7341, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 7341 (delta 7), reused 3 (delta 3), pack-reused 7330 (from 2)\u001b[K\n",
      "Receiving objects: 100% (7341/7341), 237.05 MiB | 17.55 MiB/s, done.\n",
      "Resolving deltas: 100% (5014/5014), done.\n",
      "Updating files: 100% (2285/2285), done.\n"
     ]
    }
   ],
   "source": [
    "!# Load chinese poetry dataset.\n",
    "!git clone https://github.com/chinese-poetry/chinese-poetry.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad4710e-5f97-4757-a6bb-167c3a055bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "poem_collection = []\n",
    "for file in os.listdir(\"chinese-poetry/全唐诗\"):\n",
    "    if \".json\" not in file or \"poet\" not in file:\n",
    "        continue\n",
    "    full_filename = \"%s/%s\" % (\"chinese-poetry/全唐诗\", file)\n",
    "    with open(full_filename, \"r\") as f:\n",
    "        content = json.load(f)\n",
    "        poem_collection.extend(content)\n",
    "\n",
    "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a37d63e3-6777-4ab8-b06a-80ed03797ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "貧賤元無富貴思，泥塗滑滑總危機。世無徐庶不如卧，見到淵明便合歸。流落丹心天肯未，崢嶸青眼古來稀。西風爲語巖前桂，若更多言却又非。\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed629c31-94f8-4be8-a723-3f1e908a6fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:33:56.228276: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m484/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.2525 - loss: 2.6950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:35:02.080597: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 102ms/step - accuracy: 0.2884 - loss: 2.4116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x74087ed03e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
    "    .batch(10)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Running through the whole dataset takes long, only take `500` and run 1\n",
    "# epochs for demo purposes.\n",
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-4,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "539dd181-de15-4e1b-a7df-7b75155a3499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <bound method GPT2CausalLM.generate_step of <GPT2CausalLM name=gpt2_causal_lm, built=True>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <bound method GPT2CausalLM.generate_step of <GPT2CausalLM name=gpt2_causal_lm, built=True>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨夜雨疏风骤，時自樂時爲。）江山馬山曲時。爲風爲時苑）晶山虛知素。）素虛虛，聽結更聞。\n"
     ]
    }
   ],
   "source": [
    "output = gpt2_lm.generate(\"昨夜雨疏风骤\", max_length=200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b039c-4272-449e-aa8c-bbad0afde8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c753d-0a93-4e93-9722-3cd026f5059b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb0044-24da-45f3-a0fa-aec561c226cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880e267-ed27-430e-83ac-d2b802e242a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140ba5a-511b-4293-9bd7-c036d8c6c213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c7c91-eb77-4663-b4ec-36fa24c27bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292cab4-1c59-403a-911b-1a7bd47959bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f816b-536e-4a9d-ac6b-752e31ec076f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ad340-f2dc-4900-b066-8eb130537eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a49089-cc6e-49fd-9acd-135dd7c1ebc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b454b30-f2e1-4cac-b38f-c589c8368541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84d462-afa7-4a33-98f4-5ccef769b72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
